{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ncdf4)\n",
    "library(rptha)\n",
    "ptha18 = new.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "source('../ptha/ptha_access/get_PTHA_results.R', local=ptha18, chdir=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sink(file='log_of_plot_optimal_sampling.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "source_zone = 'sunda2'\n",
    "sz_scenarios = ptha18$get_source_zone_events_data(source_zone,  slip_type='stochastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convenient shorthand for the magnitudes and rates in the event table\n",
    "event_Mw = sz_scenarios$events$Mw\n",
    "event_rates = sz_scenarios$events$rate_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Useful wrapper for extracting peak-stage values for all scenarios at a given site\n",
    "get_peak_stage_at_target_point<-function(target_point){\n",
    "    event_peak_stage_at_refpoint = ptha18$get_peak_stage_at_point_for_each_event(\n",
    "        target_point = target_point,\n",
    "        slip_type='stochastic',\n",
    "        all_source_names=source_zone)\n",
    "    return(event_peak_stage_at_refpoint[[source_zone]]$max_stage)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#24 per bin\n",
    "\n",
    "# Get the peak stage value at a point east of Tongatapu\n",
    "target_point = c(94.8929214477539, 5.541058540344238)\n",
    "event_peak_stage_target_point = get_peak_stage_at_target_point(target_point)\n",
    "\n",
    "TOTAL_SAMPLES = 600 # How many samples in the Monte-Carlo scheme?\n",
    "\n",
    "DEFAULT_MC_REPS = 10000 # When we repeat MC-sampling many times, by default this defines 'many'\n",
    "\n",
    "# If we were to make a synthetic catalogue with the same number of scenarios, how\n",
    "# many years would it cover?\n",
    "EQUIVALENT_SYNTHETIC_CATALOGUE_DURATION = (TOTAL_SAMPLES / sum(event_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# How many samples to take if we sample the same amount in all Mw bins (excluding impossible bins)?\n",
    "unique_mw = seq(7.2, 9.8, by=0.1)\n",
    "const_samples = (unique_mw < 9.65)\n",
    "const_samples = const_samples/sum(const_samples)*TOTAL_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the hazard curve, and show convergence of our estimators in the case of\n",
    "# stratified sampling with constant sampling effort in \"possible\" Mw bins\n",
    "# like fig 2 and 3 in Davies et al. (2021)\n",
    "plot_hazard_curve<-function(\n",
    "    sampling_type='stratified',\n",
    "    event_rates = sz_scenarios$events$rate_annual,\n",
    "    Number_MC_reps=DEFAULT_MC_REPS,\n",
    "    hist_xlim=c(1e-04, 5e-03),\n",
    "    event_importance_weighted_sampling_probs = NULL,\n",
    "    event_peak_stage_local = event_peak_stage_target_point,\n",
    "    mw_sampling_fun = NULL,\n",
    "    fig_title=NULL,\n",
    "    add_equivalent_synthetic_catalogue=FALSE,\n",
    "    add_95pc_analytical_CI=TRUE,\n",
    "    add_hardcoded_normal_distribution_to_second_panel = FALSE){\n",
    "\n",
    "    if(is.null(fig_title)) stop('Must provide fig title')\n",
    "\n",
    "    #\n",
    "    # Setup defaults for stratified and stratified_importance sampling\n",
    "    #\n",
    "    if(sampling_type == 'stratified'){\n",
    "\n",
    "        # No importance sampling\n",
    "        if(!is.null(event_importance_weighted_sampling_probs)){\n",
    "            stop('Do not specify event_importance_weighted_sampling_probs with stratified sampling')\n",
    "        }\n",
    "        # Weights equivalent to regular stratified sampling\n",
    "        event_importance_weighted_sampling_probs = event_rates\n",
    "\n",
    "    }else if(sampling_type == 'stratified_importance'){\n",
    "\n",
    "        # By default use stage*rate at the target point\n",
    "        if(is.null(event_importance_weighted_sampling_probs)){\n",
    "            event_importance_weighted_sampling_probs = event_rates*event_peak_stage_target_point\n",
    "        }\n",
    "\n",
    "    }else{\n",
    "        stop('unknown sampling_type')\n",
    "    }\n",
    "\n",
    "\n",
    "    if(is.null(mw_sampling_fun)){\n",
    "        # How many samples in each Mw bin? By default use a constant for 'possible' scenarios.\n",
    "        mw_sampling_fun = function(Mw){ const_samples[1]*(Mw < 9.65) }\n",
    "    }\n",
    "\n",
    "    png(fig_title, width=9, height=9, units='in', res=300)\n",
    "\n",
    "    par(mfrow=c(2,1))\n",
    "    par(mar=c(4,7,2,1))\n",
    "    options(scipen=5)\n",
    "\n",
    "    peak_stage_vals = seq(0.01, 15, by=0.01)\n",
    "    hazard_curve = sapply(peak_stage_vals,\n",
    "        function(x) sum(event_rates * (event_peak_stage_local > x)))\n",
    "    plot(peak_stage_vals, hazard_curve, log='xy', t='l', lwd=2, las=1,\n",
    "        xlab=\"\",\n",
    "        ylab='', #'Exceedance-rate (events/year)',\n",
    "        cex.lab=1.5, cex.axis=1.3, \n",
    "        ylim=c(1/10000, 1/10), xlim=c(0.02, 10))\n",
    "    mtext(side=1, \"Tsunami maximum-stage (from offshore PTHA)\", line=2.5, cex=1.5)\n",
    "    mtext(side=2, 'Exceedance-rate (events/year)', line=5.5, cex=1.5)\n",
    "    add_log_axis_ticks(side=1)\n",
    "    add_log_axis_ticks(side=2)\n",
    "    #abline(h=c(1/10, 1/25, 1/100, 1/500, 1/2500, 1/10000), col='darkgreen', lty='dashed')\n",
    "    abline(h=c(1/10, 1/100, 1/1000, 1/10000), col='darkgreen', lty='dashed')\n",
    "\n",
    "    #\n",
    "    # Compute multiple Monte-Carlo samples, store their statistics at target_stage, and plot some\n",
    "    #\n",
    "    target_stage = 1\n",
    "\n",
    "    exrate_ts_store = list(mean=rep(NA, Number_MC_reps), var=rep(NA, Number_MC_reps), analytical_mean_var=rep(NA, 2))\n",
    "\n",
    "    # Store the analytical mean/variance as well, for cross-checking\n",
    "    exrate_ts_store$analytical_mean_var = ptha18$analytical_Monte_Carlo_exrate_uncertainty(\n",
    "        event_Mw,\n",
    "        event_rates,\n",
    "        event_peak_stage_local,\n",
    "        stage_threshold=target_stage,\n",
    "        samples_per_Mw=mw_sampling_fun,\n",
    "        event_importance_weighted_sampling_probs=event_importance_weighted_sampling_probs)\n",
    "\n",
    "    set.seed(1234) # Reproducible randomness\n",
    "    for(i in 1:Number_MC_reps){\n",
    "\n",
    "        random_scenarios = ptha18$randomly_sample_scenarios_by_Mw_and_rate(\n",
    "            event_rates,\n",
    "            event_Mw,\n",
    "            samples_per_Mw = mw_sampling_fun,\n",
    "            event_importance_weighted_sampling_probs = event_importance_weighted_sampling_probs)\n",
    "\n",
    "        # For both cases this will compute the correct mean/variance\n",
    "        mean_and_variance = ptha18$estimate_exrate_uncertainty(\n",
    "            random_scenarios, event_peak_stage_local, target_stage)\n",
    "\n",
    "        exrate_ts_store$mean[i] = mean_and_variance[1]\n",
    "        exrate_ts_store$var[i] = mean_and_variance[2]\n",
    "\n",
    "        # Plot the first 500 samples (not too many to avoid cluttering the plot)\n",
    "        if(i <= 500){\n",
    "            random_scenario_stages = event_peak_stage_local[random_scenarios$inds]\n",
    "            random_hazard_curve = sapply(peak_stage_vals,\n",
    "                function(x){\n",
    "                    sum(random_scenarios$importance_sampling_scenario_rates_basic*\n",
    "                        (random_scenario_stages > x), na.rm=TRUE)\n",
    "                })\n",
    "\n",
    "            points(peak_stage_vals, random_hazard_curve, t='l', lwd=0.2, col='grey')\n",
    "        }\n",
    "    }\n",
    "    points(peak_stage_vals, hazard_curve, t='l', lwd=2) # Put the hazard curve back on (it is covered by gray curves)\n",
    "\n",
    "\n",
    "    if(!add_equivalent_synthetic_catalogue){\n",
    "        # Different legends depending on 95% analytical CIs\n",
    "        if(!add_95pc_analytical_CI){\n",
    "            legend('bottomleft', c('All scenarios in offshore PTHA',\n",
    "                   paste0('Monte-Carlo estimates (500 only)')), \n",
    "                   lty=c(1, 1), col=c('black', 'grey'), lwd=c(2, 2), cex=1.3)\n",
    "        }else{\n",
    "            legend('bottomleft', \n",
    "                   c('All scenarios in offshore PTHA',\n",
    "                       paste0('Monte-Carlo estimates (500 only)'),\n",
    "                       '95% interval (analytical)'), \n",
    "                   lty=c('solid', 'solid', 'dotdash'), col=c('black', 'grey', 'darkred'), \n",
    "                   lwd=c(2, 2, 2), cex=1.3)\n",
    "        }\n",
    "\n",
    "    }else{\n",
    "\n",
    "        # Different legends depending on 95% analytical CIs\n",
    "        if(!add_95pc_analytical_CI){\n",
    "\n",
    "            legend('bottomleft', \n",
    "                   c('All scenarios in offshore PTHA', \n",
    "                     paste0('Monte-Carlo estimates (500 only)'), \n",
    "                     'Equivalent Synthetic Catalogue 95% interval'),\n",
    "                   lty=c('solid', 'solid', 'dashed'), col=c('black', 'grey', 'darkblue'), \n",
    "                   lwd=c(2, 1, 2), cex=1.3, bg=rgb(1,1,1,alpha=0.0), bty='o', box.col=rgb(1,1,1,alpha=0.3))\n",
    "        }else{\n",
    "\n",
    "            legend('bottomleft',\n",
    "                   c('All scenarios in offshore PTHA', \n",
    "                     paste0('Monte-Carlo estimates (500 only)'), \n",
    "                     '95% interval (analytical)', \n",
    "                     'Equivalent Synthetic Catalogue 95% interval'),\n",
    "                   lty=c('solid', 'solid', 'dotdash', 'dashed'), col=c('black', 'grey', 'darkred', 'darkblue'), \n",
    "                   lwd=c(2, 1, 2, 2), cex=1.3, bg=rgb(1,1,1,alpha=0.0), bty='o', box.col=rgb(1,1,1,alpha=0.3))\n",
    "\n",
    "        }\n",
    "\n",
    "        # Include 95% intervals for a synthetic catalogue\n",
    "        equivalent_synthetic_lower = sapply(hazard_curve, function(x){\n",
    "            qpois(0.025, lambda=x*EQUIVALENT_SYNTHETIC_CATALOGUE_DURATION)/EQUIVALENT_SYNTHETIC_CATALOGUE_DURATION\n",
    "               })\n",
    "        equivalent_synthetic_upper = sapply(hazard_curve, function(x){\n",
    "            qpois(0.975, lambda=x*EQUIVALENT_SYNTHETIC_CATALOGUE_DURATION)/EQUIVALENT_SYNTHETIC_CATALOGUE_DURATION\n",
    "               })\n",
    "\n",
    "\n",
    "        # Add lines to the plot -- because it is log-log we should not use zeros -- instead use a very small\n",
    "        # threshold\n",
    "        points(peak_stage_vals, pmax(equivalent_synthetic_lower, 1e-100), t='l', col='darkblue', lty='dashed', lwd=2)\n",
    "        points(peak_stage_vals, pmax(equivalent_synthetic_upper, 1e-100), t='l', col='darkblue', lty='dashed', lwd=2)\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    abline(v=target_stage, col='purple', lwd=2)\n",
    "\n",
    "    mean_stoc = mean(exrate_ts_store$mean)\n",
    "    var_stoc = var(exrate_ts_store$mean)\n",
    "\n",
    "    # Get the 'analytical' mean and variance expected for the sampling method\n",
    "    analytical_mean_variance = ptha18$analytical_Monte_Carlo_exrate_uncertainty(\n",
    "        event_Mw, event_rates, event_peak_stage_local, stage_threshold=target_stage,\n",
    "        samples_per_Mw = mw_sampling_fun,\n",
    "        event_importance_weighted_sampling_probs=event_importance_weighted_sampling_probs)\n",
    "    mean_analytical = analytical_mean_variance[1]\n",
    "    var_analytical = analytical_mean_variance[2]\n",
    "\n",
    "    if(add_95pc_analytical_CI){\n",
    "        # Add an analytical confidence interval to the plot, using the normal approximation\n",
    "\n",
    "        lower_CI = rep(NA, length(peak_stage_vals))\n",
    "        upper_CI = rep(NA, length(peak_stage_vals))\n",
    "\n",
    "        for(i in 1:length(peak_stage_vals)){\n",
    "            tmp = ptha18$analytical_Monte_Carlo_exrate_uncertainty(\n",
    "                event_Mw, event_rates, event_peak_stage_local, stage_threshold=peak_stage_vals[i],\n",
    "                samples_per_Mw = mw_sampling_fun,\n",
    "                event_importance_weighted_sampling_probs=event_importance_weighted_sampling_probs)\n",
    "            lower_CI[i] = tmp[1] + qnorm(0.025)*sqrt(tmp[2])\n",
    "            upper_CI[i] = tmp[1] + qnorm(0.975)*sqrt(tmp[2])\n",
    "        }\n",
    "        points(peak_stage_vals, lower_CI, t='l', col='darkred', lty='dotdash', lwd=2)\n",
    "        points(peak_stage_vals, upper_CI, t='l', col='darkred', lty='dotdash', lwd=2)\n",
    "    }\n",
    "\n",
    "    #\n",
    "    # Histogram of errors at the target_stage\n",
    "    #\n",
    "\n",
    "    hist(exrate_ts_store$mean, breaks=100, freq=FALSE, main='',\n",
    "         xlab=\"\", ylab=\"\", xlim=hist_xlim, cex.axis=1.3, las=1)\n",
    "    mtext(side=1,\n",
    "          bquote(paste('Distribution of ', .(Number_MC_reps),\n",
    "              ' Monte-Carlo exceedance-rates @ ', Q^T, '=',\n",
    "              .(target_stage), ' m')),\n",
    "          line=2.8, cex=1.5)\n",
    "    mtext(side=2, 'Probability Density', line=4, cex=1.5)\n",
    "    # Add normal distribution\n",
    "    xs_local = seq(min(exrate_ts_store$mean), max(exrate_ts_store$mean), len=500)\n",
    "    points(xs_local, dnorm(xs_local, mean=mean_analytical, sd=sqrt(var_analytical)), t='l',\n",
    "           col='darkred', lty='dotdash', lwd=3)\n",
    "\n",
    "    if(!add_hardcoded_normal_distribution_to_second_panel){\n",
    "        legend('topright', \n",
    "               paste0(\"Normal distribution (analytical\\n\",\n",
    "                      \"mean and variance)\"),\n",
    "               lwd=3, col='darkred', lty='dotdash', pch=NA, cex=1.25, bty='n')\n",
    "    }else{\n",
    "        # Useful when we want to compare the spread of Monte-Carlo results with other results\n",
    "        x_vals = seq(hist_xlim[1], hist_xlim[2], len=201)\n",
    "        y_vals = dnorm(x_vals, mean=0.00122335093286598, sd=sqrt(0.0000000304813319339911))\n",
    "        points(x_vals, y_vals, t='l', col='skyblue', lwd=3, lty='dotted')\n",
    "\n",
    "        legend('topright', c(\n",
    "               paste0(\"Normal distribution (analytical\\n\",\n",
    "                      \"mean and variance)\"),\n",
    "               paste0(\"Stratified-sampling \")),\n",
    "               lwd=c(3,3), col=c('darkred', 'blue'), lty=c('dotdash', 'dotted'), \n",
    "               pch=c(NA,NA), cex=1.25, bty='n')\n",
    "    }\n",
    "    dev.off()\n",
    "\n",
    "    #\n",
    "    # Summary results\n",
    "    #\n",
    "\n",
    "    print(c('Summary results for ', sampling_type))\n",
    "    print(c('  mean_analytical: ', mean_analytical))\n",
    "    print(c('  mean_stoc : ', mean_stoc))\n",
    "    print(c('      ratio : ', mean_analytical/mean_stoc))\n",
    "    print(c('      %err  : ', (1 - mean_stoc/mean_analytical)*100))\n",
    "    print(c('  var_analytical: ', var_analytical))\n",
    "    print(c('  var_stoc : ', var_stoc))\n",
    "    print(c('     ratio : ', var_analytical/var_stoc))\n",
    "    print(c('     %err  : ', (1-var_stoc/var_analytical)*100))\n",
    "    print(c('  sd_analytical: ', sqrt(var_analytical)))\n",
    "    print(c('  sd_stoc : ', sqrt(var_stoc)))\n",
    "    print(c('    ratio : ', sqrt(var_analytical/var_stoc)))\n",
    "    print(c('    %err  : ', (1 - sqrt(var_stoc/var_analytical))*100))\n",
    "\n",
    "    # Empirical confidence interval 95% true coverage\n",
    "    coverage_CI = mean( (mean_analytical > exrate_ts_store$mean + qnorm(0.025)*sqrt(exrate_ts_store$var)) &\n",
    "                        (mean_analytical < exrate_ts_store$mean + qnorm(0.975)*sqrt(exrate_ts_store$var)) )\n",
    "    print(c('  Empirical confidence interval coverage (ideal 0.95): ', coverage_CI))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Summary results for \" \"stratified\"          \n",
      "[1] \"  mean_analytical: \" \"0.00280673490936717\"\n",
      "[1] \"  mean_stoc : \"      \"0.00280812369837695\"\n",
      "[1] \"      ratio : \"    \"0.999505438805782\"\n",
      "[1] \"      %err  : \"      \"-0.0494805906017692\"\n",
      "[1] \"  var_analytical: \"      \"0.000000168530719488556\"\n",
      "[1] \"  var_stoc : \"           \"0.000000170616735619552\"\n",
      "[1] \"     ratio : \"     \"0.987773672240174\"\n",
      "[1] \"     %err  : \"     \"-1.23776611013513\"\n",
      "[1] \"  sd_analytical: \"    \"0.000410524931628465\"\n",
      "[1] \"  sd_stoc : \"         \"0.000413057787264146\"\n",
      "[1] \"    ratio : \"      \"0.993868035626548\"\n",
      "[1] \"    %err  : \"       \"-0.616979735099954\"\n",
      "[1] \"  Empirical confidence interval coverage (ideal 0.95): \"\n",
      "[2] \"0.9311\"                                                 \n"
     ]
    }
   ],
   "source": [
    "#save the plot\n",
    "plot_hazard_curve('stratified', fig_title = 'Exceedance_rate_stratified_target_point_24.png', \n",
    "    add_equivalent_synthetic_catalogue=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Summary results for \"  \"stratified_importance\"\n",
      "[1] \"  mean_analytical: \" \"0.00280673490936717\"\n",
      "[1] \"  mean_stoc : \"      \"0.00280736903498771\"\n",
      "[1] \"      ratio : \"    \"0.999774121031956\"\n",
      "[1] \"      %err  : \"      \"-0.0225930000879782\"\n",
      "[1] \"  var_analytical: \"       \"0.0000000400864793743773\"\n",
      "[1] \"  var_stoc : \"            \"0.0000000403602251428861\"\n",
      "[1] \"     ratio : \"     \"0.993217437030152\"\n",
      "[1] \"     %err  : \"     \"-0.68288802803631\"\n",
      "[1] \"  sd_analytical: \"    \"0.000200216081707682\"\n",
      "[1] \"  sd_stoc : \"         \"0.000200898544402109\"\n",
      "[1] \"    ratio : \"      \"0.996602948535751\"\n",
      "[1] \"    %err  : \"       \"-0.340863075835807\"\n",
      "[1] \"  Empirical confidence interval coverage (ideal 0.95): \"\n",
      "[2] \"0.9421\"                                                 \n"
     ]
    }
   ],
   "source": [
    "plot_hazard_curve('stratified_importance', fig_title = 'Exceedance_rate_stratified_importance_target_point_24.png',\n",
    "    add_hardcoded_normal_distribution_to_second_panel=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Optimal sampling\n",
    "#\n",
    "\n",
    "# Which threshold-stage values should we do the computation for?\n",
    "threshold_stages = c(1, 2)\n",
    "\n",
    "# Determine the optimal number of samples for each threshold\n",
    "optimal_samples = lapply(threshold_stages, function(x){\n",
    "    ptha18$get_optimal_number_of_samples_per_Mw(\n",
    "        event_Mw,\n",
    "        event_rates,\n",
    "        event_peak_stage_target_point,\n",
    "        stage_threshold=x,\n",
    "        total_samples=TOTAL_SAMPLES) # No importance sampling\n",
    "    })\n",
    "# As above, using importance sampling\n",
    "optimal_samples_IS = lapply(threshold_stages, function(x){\n",
    "    ptha18$get_optimal_number_of_samples_per_Mw(\n",
    "        event_Mw,\n",
    "        event_rates,\n",
    "        event_peak_stage_target_point,\n",
    "        stage_threshold=x,\n",
    "        total_samples=TOTAL_SAMPLES,\n",
    "        event_importance_weighted_sampling_probs = (event_peak_stage_target_point*event_rates)) # With importance sampling\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Develop a 'compromise' optimal number of samples, by averaging the results above\n",
    "# with the constant-sampling result\n",
    "#\n",
    "sum_sampling    = optimal_samples[[1]]$Nsamples*0\n",
    "sum_sampling_IS = optimal_samples_IS[[1]]$Nsamples*0\n",
    "for(i in 1:length(optimal_samples)){\n",
    "    sum_sampling    = sum_sampling    + optimal_samples[[i]]$Nsamples\n",
    "    sum_sampling_IS = sum_sampling_IS + optimal_samples_IS[[i]]$Nsamples\n",
    "}\n",
    "mean_optimal = sum_sampling/length(optimal_samples)\n",
    "mean_optimal_IS = sum_sampling_IS/length(optimal_samples)\n",
    "\n",
    "# Proposed approach puts % weight on constant samples, and remaining weight on the mean_optimal samples\n",
    "WEIGHT_ON_CONSTANT_SAMPLES = 0.25\n",
    "r1 = (1-WEIGHT_ON_CONSTANT_SAMPLES)\n",
    "r2 = WEIGHT_ON_CONSTANT_SAMPLES\n",
    "mean_optimal_including_constant    = (r1*mean_optimal    + r2*const_samples)\n",
    "mean_optimal_IS_including_constant = (r1*mean_optimal_IS + r2*const_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Write the case with importance-sampling to a file so we can use it later\n",
    "write.csv(\n",
    "    data.frame(unique_mw=unique_mw,\n",
    "               mean_optimal_samples_IS_including_constant=mean_optimal_IS_including_constant),\n",
    "    file='Non_uniform_sampling_effort_compromise_stratifiedImportance.csv',\n",
    "    row.names=FALSE)\n",
    "\n",
    "# Sanity check -- we always have TOTAL_SAMPLES\n",
    "stopifnot(isTRUE(all.equal(sum(mean_optimal), TOTAL_SAMPLES)))\n",
    "stopifnot(isTRUE(all.equal(sum(mean_optimal_IS), TOTAL_SAMPLES)))\n",
    "stopifnot(isTRUE(all.equal(sum(mean_optimal_including_constant), TOTAL_SAMPLES)))\n",
    "stopifnot(isTRUE(all.equal(sum(mean_optimal_IS_including_constant), TOTAL_SAMPLES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convenience functions for printing stats below\n",
    "get_variances<-function(optimal_samples_i, mean_optimal){\n",
    "\n",
    "    # Get the variance if we optimizes the sampling strategy to minimise it, for this particular\n",
    "    # site and stage-threshold. This is the 'best-case' in terms of \"number of samples' but in\n",
    "    # general we wouldn't do it (because we need to consider a range of return periods)\n",
    "    variance_best = sum(optimal_samples_i$variance_numerator/optimal_samples_i$Nsamples, na.rm=TRUE)\n",
    "\n",
    "    # Get the variance if the comprimise sampling strategy is used\n",
    "    variance_chosen = sum(optimal_samples_i$variance_numerator/mean_optimal, na.rm=TRUE)\n",
    "\n",
    "    # Get the variance if we used constant sampling in all bins\n",
    "    variance_const = sum(optimal_samples_i$variance_numerator/const_samples, na.rm=TRUE)\n",
    "\n",
    "    return(list(variance_chosen=variance_chosen,\n",
    "                chosen_on_best=variance_chosen/variance_best,\n",
    "                constant_on_chosen=variance_const/variance_chosen,\n",
    "                constant_on_best=variance_const/variance_best))\n",
    "}\n",
    "print_variances<-function(optimal_samples, mean_optimal, mean_optimal_including_constant){\n",
    "\n",
    "    for(i in 1:length(optimal_samples)){\n",
    "        vars = get_variances(optimal_samples[[i]], mean_optimal)\n",
    "        print(c('No constant  ', signif(unlist(vars), 4)), width=999)\n",
    "        vars = get_variances(optimal_samples[[i]], mean_optimal_including_constant)\n",
    "        print(c('With constant', signif(unlist(vars), 4)), width=999)\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Variances (no importance sampling) with different scenario counts\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"     \"0.0000002063\"            \"1.073\"            \"1.634\"            \"1.754\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"     \"0.0000002215\"            \"1.153\"            \"1.521\"            \"1.754\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000005088\"            \"1.073\"             \"1.75\"            \"1.878\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000005593\"             \"1.18\"            \"1.592\"            \"1.878\" \n",
      "[1] \"Variances (IMPORTANCE SAMPLING) with different scenario counts\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000002612\"            \"1.048\"            \"1.534\"            \"1.608\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000002764\"            \"1.109\"             \"1.45\"            \"1.608\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000005064\"            \"1.048\"            \"1.773\"            \"1.859\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000005633\"            \"1.166\"            \"1.594\"            \"1.859\" \n"
     ]
    }
   ],
   "source": [
    "# How does the variance change with the above sampling effort, vs 'constant sampling', and\n",
    "# vs 'the best possible for the chosen threshold'\n",
    "#\n",
    "\n",
    "print('Variances (no importance sampling) with different scenario counts')\n",
    "print_variances(optimal_samples, mean_optimal, mean_optimal_including_constant)\n",
    "\n",
    "print('Variances (IMPORTANCE SAMPLING) with different scenario counts')\n",
    "print_variances(optimal_samples_IS, mean_optimal_IS, mean_optimal_IS_including_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Do some plotting of alternative optimal sampling efforts\n",
    "#\n",
    "plot_sampling_effort<-function(\n",
    "    optimal_samples, mean_optimal, mean_optimal_including_constant,\n",
    "    optimal_samples_IS, mean_optimal_IS, mean_optimal_IS_including_constant,\n",
    "    const_samples, threshold_stages){\n",
    "\n",
    "    # Various plot pars\n",
    "    PLOT_YLIM = c(0, 100)\n",
    "    BAR_GROUP = 80\n",
    "    BAR_LWD = 4\n",
    "    MIN_BAR_HT = 0*(const_samples > 0) # To show a bar can try using a small non-zero value\n",
    "    BAR_H_OFFSET = 1.5\n",
    "\n",
    "    library(cptcity)\n",
    "    COLZ = cpt(pal='cb_seq_YlOrRd_06', n=6)[3:6]\n",
    "\n",
    "    png('Optimal_sampling_effort.png', width=9, height=6, units='in', res=300)\n",
    "    par(mar=c(4,4.5,2,1))\n",
    "    par(mfrow=c(2,1))\n",
    "    # Plot without importance sampling\n",
    "    for(i in 1:length(threshold_stages)){\n",
    "        # Stagger the x location of the bars, so we fit several bars close to the\n",
    "        # desired Mw value\n",
    "\n",
    "        if(i == 1){\n",
    "\n",
    "            plot(optimal_samples[[i]]$Mw + (i-BAR_H_OFFSET)/BAR_GROUP,\n",
    "                 pmax(optimal_samples[[i]]$Nsamples, MIN_BAR_HT),\n",
    "                 t='h', lwd=BAR_LWD, lend=1, col=COLZ[i], ylim=PLOT_YLIM,\n",
    "                 xlab='', ylab='Optimal # Samples', las=1,\n",
    "                 main='Stratified sampling', cex.main=1.8, cex.lab=1.5)\n",
    "\n",
    "            mtext(\"Mw\", side=1, line=2, cex=1.5)\n",
    "            axis(side=1, at=seq(7.2, 9.6, by=0.1), labels=FALSE)\n",
    "\n",
    "        }else{\n",
    "\n",
    "            points(optimal_samples[[i]]$Mw + (i-BAR_H_OFFSET)/BAR_GROUP,\n",
    "                   pmax(optimal_samples[[i]]$Nsamples, MIN_BAR_HT),\n",
    "                   t='h', lwd=BAR_LWD, lend=1, col=COLZ[i])\n",
    "        }\n",
    "    }\n",
    "    points(optimal_samples[[1]]$Mw + (0-BAR_H_OFFSET)/BAR_GROUP,\n",
    "           pmax(const_samples, MIN_BAR_HT),\n",
    "           t='h', lwd=BAR_LWD, lend=1, col='black')\n",
    "    grid(col='orange')\n",
    "\n",
    "    # Report the variance reduction that would be obtained by optimising the\n",
    "    # number of samples for the specific threshold (independent of the\n",
    "    # mean_optimal argument)\n",
    "    legend_VR_local_optimised = unlist(lapply(optimal_samples,\n",
    "        function(x) get_variances(x, mean_optimal)$constant_on_best))\n",
    "    legend('topleft', \n",
    "           c('Equal in all bins    (VR = 1.00)', \n",
    "              paste0('Threshold = ', threshold_stages, \n",
    "              ' m   (VR = ', round(legend_VR_local_optimised ,2),')')), \n",
    "           fill=c('black', COLZ))\n",
    "\n",
    "    # Plot with importance sampling\n",
    "    for(i in 1:length(threshold_stages)){\n",
    "        # Stagger the x location of the bars, so we fit several bars close to the\n",
    "        # desired Mw value\n",
    "\n",
    "        if(i == 1){\n",
    "\n",
    "            plot(optimal_samples_IS[[i]]$Mw + (i-2.5)/BAR_GROUP,\n",
    "                 pmax(optimal_samples_IS[[i]]$Nsamples, MIN_BAR_HT),\n",
    "                 t='h', lwd=BAR_LWD, lend=1, col=COLZ[i], ylim=PLOT_YLIM,\n",
    "                 xlab='', ylab='Optimal # Samples', las=1,\n",
    "                 main='Stratified/importance-sampling', cex.main=1.8, cex.lab=1.5)\n",
    "\n",
    "            mtext(\"Mw\", side=1, line=2, cex=1.5)\n",
    "            axis(side=1, at=seq(7.2, 9.6, by=0.1), labels=FALSE)\n",
    "\n",
    "        }else{\n",
    "\n",
    "            points(optimal_samples_IS[[i]]$Mw + (i-2.5)/BAR_GROUP,\n",
    "                   pmax(optimal_samples_IS[[i]]$Nsamples, MIN_BAR_HT),\n",
    "                   t='h', lwd=BAR_LWD, lend=1, col=COLZ[i])\n",
    "        }\n",
    "    }\n",
    "    grid(col='orange')\n",
    "    points(optimal_samples_IS[[1]]$Mw + (0-2.5)/BAR_GROUP, pmax(const_samples, MIN_BAR_HT),\n",
    "           t='h', lwd=BAR_LWD, lend=1, col='black')\n",
    "\n",
    "    # Report the variance reduction that would be obtained by optimising the\n",
    "    # number of samples for the specific threshold (independent of the\n",
    "    # mean_optimal_IS)\n",
    "    legend_VR_local_optimised = unlist(lapply(optimal_samples_IS,\n",
    "        function(x) get_variances(x, mean_optimal_IS)$constant_on_best))\n",
    "    legend('topleft', \n",
    "           c('Equal in all bins    (VR = 1.00)', \n",
    "              paste0('Threshold = ', threshold_stages, \n",
    "                     ' m   (VR = ', round(legend_VR_local_optimised,2),')')), \n",
    "           fill=c('black', COLZ))\n",
    "    dev.off()\n",
    "\n",
    "\n",
    "    #\n",
    "    # Plot the chosen number of samples, and the variance reductions.\n",
    "    #\n",
    "    png('Chosen_sampling_effort.png', width=9, height=6, units='in', res=300)\n",
    "    BAR_EPS = 0.012\n",
    "    plot(optimal_samples[[1]]$Mw - BAR_EPS, mean_optimal_including_constant,\n",
    "         ylim=c(0, 100), xlim=c(7.15, 9.65),\n",
    "         t='h', lwd=BAR_LWD*1.3, lend=1, col='black',\n",
    "         xlab='Mw', ylab = '# Samples', cex.lab=1.4, cex.axis=1.3)\n",
    "\n",
    "    abline(h=const_samples[1], col='purple', lty='dashed')\n",
    "    abline(v=c(8.65, 8.75), col='red', lty='dashed')\n",
    "    points(optimal_samples[[1]]$Mw + BAR_EPS, mean_optimal_IS_including_constant, ylim=c(0, 150),\n",
    "        t='h', lwd=BAR_LWD*1.3, lend=1, col='skyblue4')\n",
    "\n",
    "    title(main='Selected non-uniform sampling effort and extra variance-reduction', cex.main=1.5)\n",
    "\n",
    "    legend_VR_chosen_IS = unlist(lapply(optimal_samples_IS,\n",
    "        function(x) get_variances(x, mean_optimal_IS_including_constant)$constant_on_chosen))\n",
    "    legend_VR_chosen = unlist(lapply(optimal_samples,\n",
    "        function(x) get_variances(x, mean_optimal_including_constant)$constant_on_chosen))\n",
    "\n",
    "    white_t = rgb(1,1,1,alpha=0.0)\n",
    "    legend('topleft', \n",
    "           paste0('Threshold = ', rep(threshold_stages, 1), \n",
    "                  ' (VR = ', format(round(legend_VR_chosen, 2)), ')'),\n",
    "           title = 'Stratified sampling', \n",
    "           box.col=white_t, cex=1.1, bg=white_t)\n",
    "    legend('top', \n",
    "           paste0('Threshold = ', rep(threshold_stages, 1), \n",
    "                  ' (VR = ', format(round(legend_VR_chosen_IS, 2)), ')'),\n",
    "           title = 'Stratified/importance-sampling', \n",
    "           text.col='skyblue4',\n",
    "           title.col='skyblue4',\n",
    "           box.col=white_t, cex=1.1, bg=white_t)\n",
    "\n",
    "    text(7.5, 26, 'Uniform sampling', col='purple', cex=1.4)\n",
    "    text(8.8, 65, 'Is this optimal\\nsample for Mw 8.7???', col='red', cex=1.2, adj=0)\n",
    "    dev.off()\n",
    "\n",
    "    return(invisible(0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘cptcity’ was built under R version 4.4.2”\n"
     ]
    }
   ],
   "source": [
    "library(cptcity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Make the plots\n",
    "plot_sampling_effort(\n",
    "    optimal_samples, mean_optimal, mean_optimal_including_constant,\n",
    "    optimal_samples_IS, mean_optimal_IS, mean_optimal_IS_including_constant,\n",
    "    const_samples, threshold_stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Study how this performs at a bunch of other points\n",
    "\n",
    "\n",
    "test_point_list = list(\n",
    "    'target_point' = target_point,\n",
    "    test_point_1 = c(94.92733001708984, 5.91058349609375),\n",
    "    test_point_2 = c(95.55675506591797, 5.82645845413208),\n",
    "    test_point_3 = c(95.27789306640625, 6.073542594909668)\n",
    "    )\n",
    "\n",
    "event_peak_stage_list = list()\n",
    "optimal_samples_list = list()\n",
    "optimal_samples_IS_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Variances (no importance sampling) with different scenario counts at target_point\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"     \"0.0000002063\"            \"1.073\"            \"1.634\"            \"1.754\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"     \"0.0000002215\"            \"1.153\"            \"1.521\"            \"1.754\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000005088\"            \"1.073\"             \"1.75\"            \"1.878\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000005593\"             \"1.18\"            \"1.592\"            \"1.878\" \n",
      "[1] \"Variances (IMPORTANCE SAMPLING) with different scenario counts at target_point\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000002612\"            \"1.048\"            \"1.534\"            \"1.608\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000002764\"            \"1.109\"             \"1.45\"            \"1.608\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000005064\"            \"1.048\"            \"1.773\"            \"1.859\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000005633\"            \"1.166\"            \"1.594\"            \"1.859\" \n",
      "[1] \"Variances (no importance sampling) with different scenario counts at test_point_1\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"              \"Inf\"              \"Inf\"                \"0\"            \"1.693\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"      \"0.000000284\"            \"1.238\"            \"1.368\"            \"1.693\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000005074\"            \"1.094\"            \"1.757\"            \"1.922\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000005582\"            \"1.203\"            \"1.597\"            \"1.922\" \n",
      "[1] \"Variances (IMPORTANCE SAMPLING) with different scenario counts at test_point_1\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"              \"Inf\"              \"Inf\"                \"0\"            \"1.542\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000004118\"             \"1.19\"            \"1.296\"            \"1.542\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000006459\"            \"1.054\"            \"1.782\"            \"1.879\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000007193\"            \"1.174\"              \"1.6\"            \"1.879\" \n",
      "[1] \"Variances (no importance sampling) with different scenario counts at test_point_2\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000003447\"            \"1.304\"            \"1.602\"            \"2.089\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"    \"0.00000003701\"              \"1.4\"            \"1.492\"            \"2.089\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000008971\"             \"1.83\"            \"1.319\"            \"2.414\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000009128\"            \"1.862\"            \"1.296\"            \"2.414\" \n",
      "[1] \"Variances (IMPORTANCE SAMPLING) with different scenario counts at test_point_2\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000005711\"            \"1.225\"            \"1.752\"            \"2.146\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000006323\"            \"1.356\"            \"1.582\"            \"2.146\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"         \"9.74e-10\"            \"1.548\"            \"1.574\"            \"2.437\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000001055\"            \"1.677\"            \"1.453\"            \"2.437\" \n",
      "[1] \"Variances (no importance sampling) with different scenario counts at test_point_3\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000004931\"            \"1.095\"            \"1.712\"            \"1.875\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"     \"0.0000000539\"            \"1.197\"            \"1.566\"            \"1.875\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"    \"0.00000001228\"            \"1.498\"            \"1.413\"            \"2.117\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"     \"0.0000000127\"             \"1.55\"            \"1.366\"            \"2.117\" \n",
      "[1] \"Variances (IMPORTANCE SAMPLING) with different scenario counts at test_point_3\"\n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000007648\"            \"1.055\"            \"1.733\"            \"1.827\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000008466\"            \"1.167\"            \"1.565\"            \"1.827\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"No constant  \"   \"0.000000001393\"            \"1.327\"            \"1.624\"            \"2.155\" \n",
      "                      variance_chosen     chosen_on_best constant_on_chosen   constant_on_best \n",
      "   \"With constant\"   \"0.000000001519\"            \"1.447\"            \"1.489\"            \"2.155\" \n"
     ]
    }
   ],
   "source": [
    "for(nm_i in names(test_point_list)){\n",
    "    #\n",
    "    # Generalisation to nearby sites?\n",
    "    event_peak_stage_list[[nm_i]] = get_peak_stage_at_target_point(test_point_list[[nm_i]])\n",
    "\n",
    "    optimal_samples_list[[nm_i]] = lapply(threshold_stages, function(x){\n",
    "        ptha18$get_optimal_number_of_samples_per_Mw(\n",
    "            event_Mw,\n",
    "            event_rates,\n",
    "            event_peak_stage_list[[nm_i]],\n",
    "            stage_threshold=x,\n",
    "            total_samples=TOTAL_SAMPLES) # No importance sampling\n",
    "        })\n",
    "\n",
    "    optimal_samples_IS_list[[nm_i]] = lapply(threshold_stages, function(x){\n",
    "        ptha18$get_optimal_number_of_samples_per_Mw(\n",
    "            event_Mw,\n",
    "            event_rates,\n",
    "            event_peak_stage_list[[nm_i]],\n",
    "            stage_threshold=x,\n",
    "            total_samples=TOTAL_SAMPLES,\n",
    "            # With importance sampling based on the previous stage\n",
    "            event_importance_weighted_sampling_probs = (event_peak_stage_target_point*event_rates))\n",
    "        })\n",
    "\n",
    "    print(paste0('Variances (no importance sampling) with different scenario counts at ', nm_i))\n",
    "    print_variances(optimal_samples_list[[nm_i]], mean_optimal, mean_optimal_including_constant)\n",
    "\n",
    "    print(paste0('Variances (IMPORTANCE SAMPLING) with different scenario counts at ', nm_i))\n",
    "    print_variances(optimal_samples_IS_list[[nm_i]], mean_optimal_IS, mean_optimal_IS_including_constant)\n",
    "\n",
    "}\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Compare analytical confidence intervals at multiple sites -- 'stratified sampling with uniform-sampling\n",
    "# effort in each bin, vs stratified/importance sampling with non-uniform sampling in each bin'\n",
    "#\n",
    "plot_curve_comparison_multiple_sites<-function(\n",
    "    event_peak_stage_list,\n",
    "    unique_mw,\n",
    "    mean_optimal_IS_including_constant,\n",
    "    event_Mw,\n",
    "    event_rates,\n",
    "    event_importance_weighted_sampling_probs,\n",
    "    nonuniform_sampling_fun,\n",
    "    uniform_sampling_fun){\n",
    "\n",
    "    par(mfrow=c(2,3))\n",
    "    par(oma=c(0, 1, 0, 0))\n",
    "    par(mar=c(4,6,2,1))\n",
    "    options(scipen=5)\n",
    "    # Target_point\n",
    "    all_titles = list()\n",
    "    all_titles$target_point = 'OH-1'\n",
    "    all_titles$test_point_1 = 'OH-2 '\n",
    "    all_titles$test_point_2 = 'OH-3'\n",
    "    all_titles$test_point_3 = 'OH-4'\n",
    "   \n",
    "\n",
    "    # Plotting order\n",
    "    names_plot_order = c('target_point', 'test_point_1', 'test_point_2', 'test_point_3')\n",
    "    # Check that the values of names_plot_order are all in event_peak_stage_list\n",
    "    stopifnot(all(is.finite(match(names_plot_order, names(event_peak_stage_list)))))\n",
    "\n",
    "    # Geometrically spaced peak-stage values at which we evaluate the curve\n",
    "    peak_stage_vals = 10^seq(-2, 1.3, by=0.025)\n",
    "\n",
    "    for(nm_i in names_plot_order){\n",
    "\n",
    "        # Construct 95% \"analytical\" confidence intervals for the Monte-Carlo exceedance-rates\n",
    "        lower_CI_stratified = rep(NA, length(peak_stage_vals))\n",
    "        upper_CI_stratified = rep(NA, length(peak_stage_vals))\n",
    "        lower_CI_stratified_importance = rep(NA, length(peak_stage_vals))\n",
    "        upper_CI_stratified_importance = rep(NA, length(peak_stage_vals))\n",
    "        variance_stratified = rep(NA, length(peak_stage_vals))\n",
    "        variance_stratified_importance = rep(NA, length(peak_stage_vals))\n",
    "\n",
    "        exrate_mean = rep(NA, length(peak_stage_vals))\n",
    "        event_peak_stage_local = event_peak_stage_list[[nm_i]]\n",
    "\n",
    "        for(i in 1:length(peak_stage_vals)){\n",
    "\n",
    "            # stratified/importance sampling, non-uniform samples in each bin\n",
    "            tmp = ptha18$analytical_Monte_Carlo_exrate_uncertainty(\n",
    "                event_Mw, event_rates, event_peak_stage_local,\n",
    "                stage_threshold=peak_stage_vals[i],\n",
    "                samples_per_Mw = nonuniform_sampling_fun,\n",
    "                # Importance sampling based on target-point\n",
    "                event_importance_weighted_sampling_probs=event_importance_weighted_sampling_probs)\n",
    "            lower_CI_stratified_importance[i] = tmp[1] + qnorm(0.025)*sqrt(tmp[2])\n",
    "            upper_CI_stratified_importance[i] = tmp[1] + qnorm(0.975)*sqrt(tmp[2])\n",
    "            exrate_mean[i] = tmp[1]\n",
    "            variance_stratified_importance[i] = tmp[2]\n",
    "\n",
    "\n",
    "            # stratified sampling, uniform samples in each bin\n",
    "            tmp = ptha18$analytical_Monte_Carlo_exrate_uncertainty(\n",
    "                event_Mw, event_rates, event_peak_stage_local,\n",
    "                stage_threshold=peak_stage_vals[i],\n",
    "                samples_per_Mw = uniform_sampling_fun,\n",
    "                event_importance_weighted_sampling_probs=(event_rates))\n",
    "            lower_CI_stratified[i] = tmp[1] + qnorm(0.025)*sqrt(tmp[2])\n",
    "            upper_CI_stratified[i] = tmp[1] + qnorm(0.975)*sqrt(tmp[2])\n",
    "            variance_stratified[i] = tmp[2]\n",
    "        }\n",
    "\n",
    "        YLIM = c(1.0e-04, 1.0e-02)\n",
    "        XLIM = c( approx(lower_CI_stratified, peak_stage_vals, xout=YLIM[2])$y,\n",
    "                  approx(upper_CI_stratified, peak_stage_vals, xout=YLIM[1])$y)\n",
    "        # Fixes for corner cases when there are pretty much no waves\n",
    "        if(!is.finite(XLIM[1])) XLIM[1] = 1.0e-02\n",
    "        if(XLIM[2] <= XLIM[1]) XLIM[2] = XLIM[1] + 1.0\n",
    "\n",
    "        # Add curves to the plot\n",
    "        plot(peak_stage_vals, exrate_mean, t='l', log='xy', xlim=XLIM, ylim=YLIM,\n",
    "             xlab=\"\", ylab=\"\",\n",
    "             cex.lab=1.4, cex.axis=1.3, las=1)\n",
    "        mtext(side=1, 'Tsunami maxima (m)', line=2.2, cex=1.1)\n",
    "        mtext(side=2, 'Exceedance-rate', line=5, cex=1.2)\n",
    "        points(peak_stage_vals, lower_CI_stratified, t='l', col='blue', lty='dashed')\n",
    "        points(peak_stage_vals, upper_CI_stratified, t='l', col='blue', lty='dashed')\n",
    "        points(peak_stage_vals, lower_CI_stratified_importance, t='l', col='red', lty='twodash')\n",
    "        points(peak_stage_vals, upper_CI_stratified_importance, t='l', col='red', lty='twodash')\n",
    "        #abline(v=c(2,4), lwd=2, col='purple')\n",
    "        title(all_titles[[nm_i]], cex.main=1.8)\n",
    "        add_log_axis_ticks(side=1)\n",
    "        add_log_axis_ticks(side=2)\n",
    "        abline(h=c(1.0e-04, 1.0e-03, 1.0e-02), lty='dotted', col='orange')\n",
    "        abline(v=c(0.05, 0.1, 0.5, 1, 5, 10), lty='dotted', col='orange')\n",
    "\n",
    "        # Estimate variance reductions (interpolate with log-transform)\n",
    "        variance_reduction_at_2m = (exp(approx(peak_stage_vals, log(variance_stratified), xout=2)$y)/\n",
    "                                    exp(approx(peak_stage_vals, log(variance_stratified_importance), xout=2)$y))\n",
    "        variance_reduction_at_4m = (exp(approx(peak_stage_vals, log(variance_stratified), xout=4)$y)/\n",
    "                                    exp(approx(peak_stage_vals, log(variance_stratified_importance), xout=4)$y))\n",
    "        text(XLIM[1]*2.5, 3e-04, paste0('VR1 = ', signif(variance_reduction_at_2m, 3)), cex=1.5)\n",
    "        text(XLIM[1]*2.5, 1.33e-04, paste0('VR2 = ', signif(variance_reduction_at_4m, 3)), cex=1.5)\n",
    "    }\n",
    "\n",
    "    # Add legend in the final panel\n",
    "    par(mar=c(0,0,0,0))\n",
    "    plot(c(0,1), c(0,1), ann=FALSE, col='white', axes=FALSE)\n",
    "    legend(0.00, 1.0, c('95% interval (analytical) \\nStratified with uniform\\nMw-bin sampling'),\n",
    "           lty='dashed', col='blue', lwd=2, cex=1.4, bty='n')\n",
    "    legend(0.00, 0.7, c('95% interval (analytical) \\nStratified/importance with\\nnon-uniform Mw-bin sampling'),\n",
    "           lty='twodash', col='red', lwd=2, cex=1.4, bty='n')\n",
    "    text(0.5, 0.25, bquote(paste('VR1 = Variance-reduction @ ', Q^T, '=1m')), cex=1.4)\n",
    "    text(0.5, 0.15, bquote(paste('VR2 = Variance-reduction @ ', Q^T, '=2m')), cex=1.4) \n",
    "\n",
    "}\n",
    "# Compare the scheme used herein with 'stratified sampling and uniform N(M_w)', using the logic-tree mean\n",
    "# kermadectonga2 Mw-frequency curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in xy.coords(x, y, xlabel, ylabel, log):\n",
      "“12 y values <= 0 omitted from logarithmic plot”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in xy.coords(x, y, xlabel, ylabel, log):\n",
      "“5 y values <= 0 omitted from logarithmic plot”\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png('Curve_comparison_multiple_sites.png', width=9, height=4.5, units='in', res=300)\n",
    "plot_curve_comparison_multiple_sites(\n",
    "    event_peak_stage_list,\n",
    "    unique_mw,\n",
    "    mean_optimal_IS_including_constant,\n",
    "    event_Mw,\n",
    "    event_rates,\n",
    "    event_importance_weighted_sampling_probs=(event_peak_stage_target_point*event_rates),\n",
    "    nonuniform_sampling_fun = approxfun(unique_mw, mean_optimal_IS_including_constant, method='constant'),\n",
    "    uniform_sampling_fun = function(Mw){ const_samples[1]*(Mw < 9.65) }\n",
    "    )\n",
    "dev.off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# As above, but do the calculation for the unsegmented/segmented logic-tree mean, and assume that stratified\n",
    "# sampling can spend 50% of the scenarios on the unsegmented branch. Later we also look at the segments, assuming\n",
    "# we can spend 30% on Tonga, 20% on Kermadec, and 10% on Hikurangi. In a real application the latter numbers\n",
    "# should add to 50%, but these plots are just exploratory - we never use the combined results - so there is no\n",
    "# problem.\n",
    "#\n",
    "\n",
    "ptha18_source_rate_env = new.env()\n",
    "#source('../../../../../AustPTHA/CODE/ptha/ptha_access/get_detailed_PTHA18_source_zone_info.R',\n",
    "source('../ptha/ptha_access/get_detailed_PTHA18_source_zone_info.R',  local=ptha18_source_rate_env, chdir=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unsegmented case -- stratified sampling gets 50% of the total scenarios (other 50% on segments)\n",
    "source_zone = 'sunda2'\n",
    "unsegmented_sz = ptha18_source_rate_env$get_PTHA18_scenario_conditional_probability_and_rates_on_segment(source_zone, segment='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in xy.coords(x, y, xlabel, ylabel, log):\n",
      "“12 y values <= 0 omitted from logarithmic plot”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in xy.coords(x, y, xlabel, ylabel, log):\n",
      "“5 y values <= 0 omitted from logarithmic plot”\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png('Curve_comparison_multiple_sites_UNSEGMENTED.png', width=9, height=4.5, units='in', res=300)\n",
    "plot_curve_comparison_multiple_sites(event_peak_stage_list,unique_mw,mean_optimal_IS_including_constant,event_Mw,\n",
    "    # Assume the true exceedance-rates correspond to the UNSEGMENTED logic-tree mean\n",
    "    event_rates=unsegmented_sz$HS_event_rates,event_importance_weighted_sampling_probs=(event_peak_stage_target_point*event_rates),\n",
    "    nonuniform_sampling_fun = approxfun(unique_mw, mean_optimal_IS_including_constant, method='constant'),\n",
    "    # Assume for stratified sampling that only half of the samples are spent on the unsegmented model.\n",
    "    # The other half would have to be used to sample the segments\n",
    "    uniform_sampling_fun = function(Mw){ 0.5*const_samples[1]*(Mw < 9.65) }\n",
    "    )\n",
    "dev.off()\n",
    "# Check it works OK via the other plot (which shows MC samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Summary results for \"  \"stratified_importance\"\n",
      "[1] \"  mean_analytical: \" \"0.00229634915930219\"\n",
      "[1] \"  mean_stoc : \"      \"0.00229635964207239\"\n",
      "[1] \"      ratio : \"    \"0.999995435048584\"\n",
      "[1] \"      %err  : \"        \"-0.000456497225509445\"\n",
      "[1] \"  var_analytical: \"       \"0.0000000167984116368312\"\n",
      "[1] \"  var_stoc : \"            \"0.0000000166736637737495\"\n",
      "[1] \"     ratio : \"    \"1.00748173075663\"\n",
      "[1] \"     %err  : \"     \"0.742617015100444\"\n",
      "[1] \"  sd_analytical: \"    \"0.000129608686579377\"\n",
      "[1] \"  sd_stoc : \"         \"0.000129126541709091\"\n",
      "[1] \"    ratio : \"     \"1.00373389439464\"\n",
      "[1] \"    %err  : \"      \"0.372000429146646\"\n",
      "[1] \"  Empirical confidence interval coverage (ideal 0.95): \"\n",
      "[2] \"0.9447\"                                                 \n"
     ]
    }
   ],
   "source": [
    "plot_hazard_curve('stratified_importance', event_rates=unsegmented_sz$HS_event_rates,\n",
    "    Number_MC_reps=10000,\n",
    "    event_importance_weighted_sampling_probs=event_rates*event_peak_stage_target_point,\n",
    "    fig_title='target_point_stratified_importance_unequal_UNSEGMENTED.png',\n",
    "    mw_sampling_fun=approxfun(unique_mw, mean_optimal_IS_including_constant, method='constant'))\n",
    "\n",
    "# As above, but do the calculation for the Tonga segment logic-tree mean, and assume that stratified\n",
    "# sampling can spend 30% of the scenarios on the Tonga segment (optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# As above, but do the calculation for the Tonga segment logic-tree mean, and assume that stratified\n",
    "# sampling can spend 30% of the scenarios on the Andaman segment (optimistic)\n",
    "\n",
    "\n",
    "sz_andaman_ = ptha18_source_rate_env$get_PTHA18_scenario_conditional_probability_and_rates_on_segment(\n",
    "    source_zone='sunda2', segment='andaman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in xy.coords(x, y, xlabel, ylabel, log):\n",
      "“12 y values <= 0 omitted from logarithmic plot”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "“collapsing to unique 'x' values”\n",
      "Warning message in xy.coords(x, y, xlabel, ylabel, log):\n",
      "“5 y values <= 0 omitted from logarithmic plot”\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png('Curve_comparison_multiple_sites_Andaman_SEGMENT.png', width=9, height=4.5, units='in', res=300)\n",
    "plot_curve_comparison_multiple_sites(\n",
    "    event_peak_stage_list,\n",
    "    unique_mw,\n",
    "    mean_optimal_IS_including_constant,\n",
    "    event_Mw,\n",
    "    # Assume the true exceedance-rates correspond to the tonga-segment logic-tree mean\n",
    "    event_rates=sz_andaman_$HS_event_rates,\n",
    "    event_importance_weighted_sampling_probs=(event_peak_stage_target_point*event_rates),\n",
    "    nonuniform_sampling_fun = approxfun(unique_mw, mean_optimal_IS_including_constant, method='constant'),\n",
    "    # Assume for stratified sampling that only half of the samples are spent on the unsegmented model.\n",
    "    # The other half would have to be used to sample the segments\n",
    "    uniform_sampling_fun = function(Mw){ 0.125*const_samples[1]*(Mw < 9.65) }\n",
    "    )\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Summary results for \"  \"stratified_importance\"\n",
      "[1] \"  mean_analytical: \" \"0.00293318641562153\"\n",
      "[1] \"  mean_stoc : \"      \"0.00293197518359069\"\n",
      "[1] \"      ratio : \"   \"1.00041311128335\"\n",
      "[1] \"      %err  : \"    \"0.041294069288822\"\n",
      "[1] \"  var_analytical: \"       \"0.0000000404155901255423\"\n",
      "[1] \"  var_stoc : \"            \"0.0000000402013563340566\"\n",
      "[1] \"     ratio : \"    \"1.00532901899392\"\n",
      "[1] \"     %err  : \"     \"0.530077108413496\"\n",
      "[1] \"  sd_analytical: \"  \"0.0002010362905685\"\n",
      "[1] \"  sd_stoc : \"         \"0.000200502758918815\"\n",
      "[1] \"    ratio : \"     \"1.00266096911864\"\n",
      "[1] \"    %err  : \"      \"0.265390715365754\"\n",
      "[1] \"  Empirical confidence interval coverage (ideal 0.95): \"\n",
      "[2] \"0.9459\"                                                 \n"
     ]
    }
   ],
   "source": [
    "# Check it works OK via the other plot (which shows MC samples)\n",
    "plot_hazard_curve('stratified_importance', event_rates=sz_andaman_$HS_event_rates,\n",
    "    Number_MC_reps=10000,\n",
    "    event_importance_weighted_sampling_probs=event_rates*event_peak_stage_target_point,\n",
    "    fig_title='target_point_stratified_importance_unequal_Andaman_SEGMENT.png',\n",
    "    mw_sampling_fun=approxfun(unique_mw, mean_optimal_IS_including_constant, method='constant'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
